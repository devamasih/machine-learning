{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression for predicting insurance claim cost\n",
    "\n",
    "In order to offer competitive prices and hence attract customers to an otherwise undifferentiated product, insurers need to be able to predict how much any given insurance claim will cost them.\n",
    "They also need to predict the cost of claims in advance so that they can offer their insurance packages without the risk of making a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd    # for reading csv data\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the dataset\n",
    "\n",
    "This dataset constists of \n",
    "\n",
    "rename the data file to `claim_costs.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.48831226e+01 6.61797085e+00 4.61127054e+01 ... 2.14450883e+00\n",
      "  1.37281866e+01 4.14286368e+03]\n",
      " [7.32642738e+01 9.97246823e+01 8.17092345e+01 ... 9.46795876e+01\n",
      "  3.45941237e+01 8.34806742e+03]\n",
      " [6.75670459e+00 6.29765119e+01 4.58118575e+01 ... 2.84445387e+01\n",
      "  2.94493562e+01 4.94751565e+03]\n",
      " ...\n",
      " [4.01979770e+01 2.92544351e+01 4.86059768e+01 ... 2.12417734e+01\n",
      "  6.19442140e+01 7.81426029e+03]\n",
      " [1.37961192e+01 7.34018502e+01 1.55845465e+01 ... 6.50788691e+00\n",
      "  8.85812544e+01 8.65646838e+03]\n",
      " [4.82804091e+01 4.76625752e+01 2.31347373e+01 ... 7.35452927e+01\n",
      "  8.18142284e+01 9.09558940e+03]]\n",
      "(321, 7)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('claim_costs.csv')\n",
    "#data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "data = np.array(data)     # convert to matrix\n",
    "\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.56608880e-01 -4.02556176e-01 -2.51665483e-02 -3.14549158e-01\n",
      "  -5.06589672e-01 -3.62774752e-01]\n",
      " [ 2.38163618e-01  5.31369372e-01  3.34111445e-01  2.42400320e-01\n",
      "   4.23818305e-01 -1.53736255e-01]\n",
      " [-4.33854741e-01  1.62759476e-01 -2.82030256e-02 -1.28739846e-01\n",
      "  -2.42152035e-01 -2.05277408e-01]\n",
      " ...\n",
      " [-9.59511122e-02 -1.75496580e-01 -1.80488000e-06 -4.54418272e-03\n",
      "  -3.14573331e-01  1.20261590e-01]\n",
      " [-3.62725757e-01  2.67332924e-01 -3.33289165e-01  9.29868675e-02\n",
      "  -4.62717426e-01  3.87115979e-01]\n",
      " [-1.42830733e-02  9.14996194e-03 -2.57084617e-01  1.31357867e-01\n",
      "   2.11320333e-01  3.19322766e-01]]\n",
      "(321, 6)\n",
      "(321, 1)\n"
     ]
    }
   ],
   "source": [
    "# CENTER AROUND MEAN\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "features -= features.mean(axis=0)\n",
    "\n",
    "# DIVIDE BY RANGE\n",
    "max_features = features.max(axis=0)\n",
    "min_features = features.min(axis=0)\n",
    "\n",
    "ranges = max_features - min_features\n",
    "\n",
    "features /= ranges\n",
    "print(features)\n",
    "\n",
    "print(features.shape)\n",
    "labels = np.reshape(labels, (features.shape[0], 1))\n",
    "print(labels.shape)\n",
    "\n",
    "normalised_data = np.hstack((features, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slitting the dataset into test, train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "(256, 7)\n",
      "(48, 7)\n",
      "(17, 7)\n"
     ]
    }
   ],
   "source": [
    "n_data = normalised_data\n",
    "shuffle(n_data)\n",
    "\n",
    "print(len(n_data))\n",
    "\n",
    "train_size =int( 0.8 * len(n_data) )\n",
    "val_size = int( 0.15 * len(n_data) )\n",
    "test_size = len(n_data) - train_size - val_size\n",
    "\n",
    "train_data = n_data[ : train_size]\n",
    "val_data = n_data[train_size : train_size + val_size]\n",
    "test_data = n_data[train_size + val_size :]\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.batches = []\n",
    "        i = 0\n",
    "        while i + batch_size < len(dataset):\n",
    "            self.batches.append(dataset[i:i+batch_size])\n",
    "            i += batch_size\n",
    "        self.batches.append(dataset[i:])\n",
    "        #shuffle(self.batches)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            shuffle(self.batches)\n",
    "        #print(type(self.batches))\n",
    "        batch = self.batches[idx]\n",
    "        features = batch[:, :-1]\n",
    "        labels = batch[:, -1]\n",
    "        return features, labels\n",
    "\n",
    "train_loader = DataLoader(train_data, 5)\n",
    "val_loader = DataLoader(val_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "We are going to creat a function to map our inputs to our output (confidence of transaction being false)\n",
    "We will use a linear model as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "class LinearModel():\n",
    "    \n",
    "    def __init__(self, n_features=6, n_outputs=1):\n",
    "        self.weights = np.random.randn(n_outputs, n_features)    \n",
    "        self.biases = np.random.randn(n_outputs)\n",
    "        print(self.weights.shape)\n",
    "        print(self.biases.shape)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.matmul(self.weights, x.T)\n",
    "        x += self.biases\n",
    "        return x\n",
    "    \n",
    "linear_model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the loss function\n",
    "\n",
    "The loss function is a measure of how badly the model is currently performing. \n",
    "\n",
    "# $L = \\frac{1}{2m} \\sum^m_{i=1} (\\hat{y} - y)^2 = \\frac{1}{2m} \\sum^m_{i=1} ( (WX + b) - y)^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, batch, gradient=False):\n",
    "    x, y = batch\n",
    "    #print('y:', y)\n",
    "    #print(y.shape)\n",
    "    \n",
    "    y_hat = model(x)\n",
    "    \n",
    "    #print('y_hat:', y_hat)\n",
    "    #print(y_hat.shape)\n",
    "    \n",
    "    if gradient == False:\n",
    "        loss = 0.5 * np.mean( np.power( (y_hat - y), 2 ) )\n",
    "        print(loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    if gradient == True:\n",
    "        z = model(x)\n",
    "        print('z', (z-y).shape)\n",
    "        print(x.shape)\n",
    "        \n",
    "        w_grad = (np.matmul( ( z - y ), x ) )\n",
    "        print(w_grad)\n",
    "        \n",
    "        b_grad = np.mean( z - y )\n",
    "        return w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the optimiser\n",
    "The optimiser will update the parameters (weights and biases) of the model in a direction that reduces the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimiser():\n",
    "    \n",
    "    def __init__(self, model, lr=0.1):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self, grad):\n",
    "        w_grad, b_grad = grad\n",
    "        self.model.weights -= self.lr * w_grad        \n",
    "        self.model.biases -= self.lr * b_grad\n",
    "        \n",
    "optimiser = SGDOptimiser(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0043796832348874e-07\n",
      "Epoch: 0 \tBatch: 0 \tLoss: 1.0043796832348874e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.08864216e-04 -4.67807159e-05 -3.15027275e-04 -2.71095520e-05\n",
      "   1.11595368e-04 -2.40572854e-04]]\n",
      "Validating\n",
      "1.9247924147140866e-07\n",
      "val_loss: 1.9247924147140866e-07\n",
      "2.6129701390306127e-07\n",
      "val_loss: 2.6129701390306127e-07\n",
      "1.0158455128439571e-07\n",
      "val_loss: 1.0158455128439571e-07\n",
      "3.937486302687592e-07\n",
      "val_loss: 3.937486302687592e-07\n",
      "4.2843911290254537e-07\n",
      "val_loss: 4.2843911290254537e-07\n",
      "2.709630331762568e-07\n",
      "val_loss: 2.709630331762568e-07\n",
      "3.1146462297505366e-07\n",
      "val_loss: 3.1146462297505366e-07\n",
      "7.363024067714255e-08\n",
      "val_loss: 7.363024067714255e-08\n",
      "3.880202797545977e-07\n",
      "val_loss: 3.880202797545977e-07\n",
      "4.0301831867800125e-08\n",
      "val_loss: 4.0301831867800125e-08\n",
      "2.562514730500461e-07\n",
      "Epoch: 0 \tBatch: 1 \tLoss: 2.562514730500461e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.27438468e-04  5.98785044e-04 -9.19314177e-04  8.67431540e-04\n",
      "  -3.86495837e-05 -2.84402291e-04]]\n",
      "4.07967595940366e-08\n",
      "Epoch: 0 \tBatch: 2 \tLoss: 4.07967595940366e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00043352 -0.00024487  0.00010348  0.00017827  0.000317    0.0001206 ]]\n",
      "5.06540017648244e-08\n",
      "Epoch: 0 \tBatch: 3 \tLoss: 5.06540017648244e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 5.84739741e-05  3.94595834e-04  1.96268203e-04  3.18521977e-04\n",
      "   2.67976738e-04 -2.12963571e-04]]\n",
      "6.193185692429696e-07\n",
      "Epoch: 0 \tBatch: 4 \tLoss: 6.193185692429696e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00219052 -0.00206728 -0.0001767  -0.00152305 -0.00253799 -0.00186071]]\n",
      "1.871027983459788e-08\n",
      "Epoch: 0 \tBatch: 5 \tLoss: 1.871027983459788e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1.16381015e-04  1.84924332e-04  3.25868510e-05 -3.44111987e-05\n",
      "   1.46424644e-04 -2.71647838e-04]]\n",
      "Validating\n",
      "7.318696932101781e-08\n",
      "val_loss: 7.318696932101781e-08\n",
      "3.219007356974624e-07\n",
      "val_loss: 3.219007356974624e-07\n",
      "3.1805864717713655e-07\n",
      "val_loss: 3.1805864717713655e-07\n",
      "2.0092397113290736e-07\n",
      "val_loss: 2.0092397113290736e-07\n",
      "6.51408647488576e-08\n",
      "val_loss: 6.51408647488576e-08\n",
      "2.4248871150695444e-07\n",
      "val_loss: 2.4248871150695444e-07\n",
      "8.836659543975505e-08\n",
      "val_loss: 8.836659543975505e-08\n",
      "1.621434451176887e-07\n",
      "val_loss: 1.621434451176887e-07\n",
      "2.3856386774122555e-07\n",
      "val_loss: 2.3856386774122555e-07\n",
      "1.7269336098042952e-07\n",
      "val_loss: 1.7269336098042952e-07\n",
      "2.334481688834858e-07\n",
      "Epoch: 0 \tBatch: 6 \tLoss: 2.334481688834858e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.000711    0.00033418 -0.00095815 -0.00038541 -0.00015486 -0.0005733 ]]\n",
      "8.381726468541434e-08\n",
      "Epoch: 0 \tBatch: 7 \tLoss: 8.381726468541434e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 4.50217713e-04 -3.35097184e-04 -5.48557182e-04  2.12072352e-04\n",
      "   2.52677474e-04  4.69271892e-05]]\n",
      "1.6480559003432042e-07\n",
      "Epoch: 0 \tBatch: 8 \tLoss: 1.6480559003432042e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 8.44145648e-04 -3.66745820e-04 -4.95251813e-04 -4.51901693e-05\n",
      "   1.33762343e-05 -1.98574312e-04]]\n",
      "2.45447235433835e-07\n",
      "Epoch: 0 \tBatch: 9 \tLoss: 2.45447235433835e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00021744  0.00056687 -0.00063176  0.00056907  0.00040791 -0.00083816]]\n",
      "5.8062111266476896e-08\n",
      "Epoch: 0 \tBatch: 10 \tLoss: 5.8062111266476896e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 9.99436249e-05  2.85233223e-04  1.91428682e-04  2.65697735e-04\n",
      "   3.20496220e-04 -2.56075433e-04]]\n",
      "Validating\n",
      "2.1730674102000863e-07\n",
      "val_loss: 2.1730674102000863e-07\n",
      "1.7162606158036126e-07\n",
      "val_loss: 1.7162606158036126e-07\n",
      "4.453263769023451e-08\n",
      "val_loss: 4.453263769023451e-08\n",
      "1.3066277755001822e-07\n",
      "val_loss: 1.3066277755001822e-07\n",
      "2.7490940825203623e-08\n",
      "val_loss: 2.7490940825203623e-08\n",
      "1.5777962754349914e-07\n",
      "val_loss: 1.5777962754349914e-07\n",
      "1.6786095436400594e-07\n",
      "val_loss: 1.6786095436400594e-07\n",
      "3.960847714269338e-08\n",
      "val_loss: 3.960847714269338e-08\n",
      "2.838890875993537e-07\n",
      "val_loss: 2.838890875993537e-07\n",
      "1.2450968208069114e-07\n",
      "val_loss: 1.2450968208069114e-07\n",
      "2.9045076336679447e-08\n",
      "Epoch: 0 \tBatch: 11 \tLoss: 2.9045076336679447e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.62515776e-04  2.05289868e-05 -2.49703320e-05 -2.29981899e-04\n",
      "   2.17932893e-05 -1.78850396e-04]]\n",
      "1.5733594678330157e-07\n",
      "Epoch: 0 \tBatch: 12 \tLoss: 1.5733594678330157e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 4.28927777e-04 -2.27388487e-04  7.69097706e-05  1.96593232e-04\n",
      "  -4.35803239e-04 -5.36566868e-04]]\n",
      "1.2143786162275655e-07\n",
      "Epoch: 0 \tBatch: 13 \tLoss: 1.2143786162275655e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 5.26140136e-04  4.52384519e-04  2.25665799e-04  2.22931094e-05\n",
      "   9.52861440e-05 -3.37885332e-04]]\n",
      "1.1996527763303746e-07\n",
      "Epoch: 0 \tBatch: 14 \tLoss: 1.1996527763303746e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00039158 -0.00036315 -0.00019208  0.00056777 -0.00032069 -0.00037696]]\n",
      "9.723952483354234e-08\n",
      "Epoch: 0 \tBatch: 15 \tLoss: 9.723952483354234e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00039425  0.0005603  -0.00030594  0.00031474  0.00026879 -0.00016153]]\n",
      "Validating\n",
      "2.0818910467180175e-08\n",
      "val_loss: 2.0818910467180175e-08\n",
      "3.486579283842308e-08\n",
      "val_loss: 3.486579283842308e-08\n",
      "8.976128659517279e-08\n",
      "val_loss: 8.976128659517279e-08\n",
      "1.0187831436978853e-07\n",
      "val_loss: 1.0187831436978853e-07\n",
      "1.26444266349723e-07\n",
      "val_loss: 1.26444266349723e-07\n",
      "2.1123561897006866e-07\n",
      "val_loss: 2.1123561897006866e-07\n",
      "1.6322990893257644e-07\n",
      "val_loss: 1.6322990893257644e-07\n",
      "1.1946309766259673e-07\n",
      "val_loss: 1.1946309766259673e-07\n",
      "1.3184709402484946e-07\n",
      "val_loss: 1.3184709402484946e-07\n",
      "3.078563845817957e-08\n",
      "val_loss: 3.078563845817957e-08\n",
      "9.083975482948592e-08\n",
      "Epoch: 0 \tBatch: 16 \tLoss: 9.083975482948592e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.79939234e-04  5.08631048e-05  9.58150228e-06 -2.60842744e-04\n",
      "  -2.01316924e-04 -5.00898554e-04]]\n",
      "9.361096325151842e-08\n",
      "Epoch: 0 \tBatch: 17 \tLoss: 9.361096325151842e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00028805 -0.00013838  0.00013058  0.00042282 -0.00039215 -0.00033436]]\n",
      "9.34189185177836e-08\n",
      "Epoch: 0 \tBatch: 18 \tLoss: 9.34189185177836e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.65065457e-05  6.75529969e-04  9.96492242e-05  1.43864886e-05\n",
      "  -2.84337837e-04 -4.76994391e-04]]\n",
      "6.88515430705995e-08\n",
      "Epoch: 0 \tBatch: 19 \tLoss: 6.88515430705995e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 4.24074516e-05 -3.38978891e-04 -4.61081567e-04  3.58515283e-04\n",
      "   2.01751246e-04 -4.84635752e-04]]\n",
      "7.207672174353867e-08\n",
      "Epoch: 0 \tBatch: 20 \tLoss: 7.207672174353867e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00027344  0.00046545  0.00038438  0.00024712  0.00016866 -0.00039037]]\n",
      "Validating\n",
      "1.465835725969907e-07\n",
      "val_loss: 1.465835725969907e-07\n",
      "1.251202112592291e-07\n",
      "val_loss: 1.251202112592291e-07\n",
      "1.8118319894056213e-08\n",
      "val_loss: 1.8118319894056213e-08\n",
      "9.778805336619092e-08\n",
      "val_loss: 9.778805336619092e-08\n",
      "7.296614923524951e-08\n",
      "val_loss: 7.296614923524951e-08\n",
      "1.9705506299974712e-08\n",
      "val_loss: 1.9705506299974712e-08\n",
      "8.481421998350984e-08\n",
      "val_loss: 8.481421998350984e-08\n",
      "2.775370005771421e-08\n",
      "val_loss: 2.775370005771421e-08\n",
      "6.768429000748828e-08\n",
      "val_loss: 6.768429000748828e-08\n",
      "9.30085228690251e-08\n",
      "val_loss: 9.30085228690251e-08\n",
      "5.2202052898128036e-08\n",
      "Epoch: 0 \tBatch: 21 \tLoss: 5.2202052898128036e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.45229385e-04  3.40624114e-04 -7.63849472e-06  1.91967753e-04\n",
      "  -1.74455046e-04 -8.75678990e-05]]\n",
      "4.631155603947381e-08\n",
      "Epoch: 0 \tBatch: 22 \tLoss: 4.631155603947381e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.04066069e-04  3.95735929e-04  1.36112144e-04  1.80288599e-04\n",
      "   7.19514244e-05 -1.66497118e-04]]\n",
      "1.4488347292912627e-07\n",
      "Epoch: 0 \tBatch: 23 \tLoss: 1.4488347292912627e-07\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00064124  0.00036265 -0.00036435  0.0004786   0.00016431 -0.00044271]]\n",
      "5.750873012907226e-08\n",
      "Epoch: 0 \tBatch: 24 \tLoss: 5.750873012907226e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00010431 -0.00026518 -0.00020294  0.00014891 -0.00014953 -0.0004304 ]]\n",
      "8.112701185616701e-09\n",
      "Epoch: 0 \tBatch: 25 \tLoss: 8.112701185616701e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 5.53923338e-05 -9.52901409e-05 -1.12101700e-04  1.40156549e-04\n",
      "   4.86920251e-05  1.87925606e-05]]\n",
      "Validating\n",
      "1.05802297862421e-08\n",
      "val_loss: 1.05802297862421e-08\n",
      "5.4055424903385897e-08\n",
      "val_loss: 5.4055424903385897e-08\n",
      "7.415864716510692e-09\n",
      "val_loss: 7.415864716510692e-09\n",
      "1.1479717133534614e-07\n",
      "val_loss: 1.1479717133534614e-07\n",
      "7.399375916687704e-08\n",
      "val_loss: 7.399375916687704e-08\n",
      "6.163783586431958e-08\n",
      "val_loss: 6.163783586431958e-08\n",
      "7.05084124313829e-08\n",
      "val_loss: 7.05084124313829e-08\n",
      "1.6219187495539018e-08\n",
      "val_loss: 1.6219187495539018e-08\n",
      "7.933531930381113e-08\n",
      "val_loss: 7.933531930381113e-08\n",
      "5.109517689298974e-08\n",
      "val_loss: 5.109517689298974e-08\n",
      "1.558469880823976e-08\n",
      "Epoch: 0 \tBatch: 26 \tLoss: 1.558469880823976e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.91727849e-05 -1.03128934e-04  4.85879275e-06  8.51229177e-05\n",
      "   2.16848159e-04 -2.24831503e-04]]\n",
      "4.0870554498975707e-08\n",
      "Epoch: 0 \tBatch: 27 \tLoss: 4.0870554498975707e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.74054422e-04 -7.30266526e-05  1.27149147e-04  3.29934520e-04\n",
      "   5.61425746e-05 -1.89957153e-04]]\n",
      "3.8298585041155254e-08\n",
      "Epoch: 0 \tBatch: 28 \tLoss: 3.8298585041155254e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-9.45373279e-05  3.05935712e-04 -2.13877833e-05  1.36998642e-04\n",
      "  -1.93500607e-04 -3.09392660e-04]]\n",
      "4.2325030347616465e-08\n",
      "Epoch: 0 \tBatch: 29 \tLoss: 4.2325030347616465e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.77038420e-04 -2.39264506e-04 -4.20528579e-05 -1.24086661e-04\n",
      "  -3.12216638e-04 -2.16638434e-04]]\n",
      "5.079235730202894e-08\n",
      "Epoch: 0 \tBatch: 30 \tLoss: 5.079235730202894e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-7.92440779e-06  2.16117892e-04 -2.94721241e-04  1.54700880e-04\n",
      "  -8.21699081e-05 -4.33360194e-04]]\n",
      "Validating\n",
      "5.4882843531987495e-08\n",
      "val_loss: 5.4882843531987495e-08\n",
      "5.939717944839263e-08\n",
      "val_loss: 5.939717944839263e-08\n",
      "5.340172966933503e-09\n",
      "val_loss: 5.340172966933503e-09\n",
      "8.857044525184777e-08\n",
      "val_loss: 8.857044525184777e-08\n",
      "1.1782530488618785e-08\n",
      "val_loss: 1.1782530488618785e-08\n",
      "5.33854738363513e-08\n",
      "val_loss: 5.33854738363513e-08\n",
      "5.217340457362119e-09\n",
      "val_loss: 5.217340457362119e-09\n",
      "3.888731048500794e-08\n",
      "val_loss: 3.888731048500794e-08\n",
      "4.1270067915875686e-08\n",
      "val_loss: 4.1270067915875686e-08\n",
      "4.5578629496046335e-08\n",
      "val_loss: 4.5578629496046335e-08\n",
      "4.228029914086027e-09\n",
      "Epoch: 0 \tBatch: 31 \tLoss: 4.228029914086027e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 6.99562527e-05 -4.94592415e-05  1.20089134e-04  5.12107846e-05\n",
      "  -7.22895600e-05  1.08747529e-05]]\n",
      "7.226805184550995e-09\n",
      "Epoch: 0 \tBatch: 32 \tLoss: 7.226805184550995e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-2.03924834e-05 -9.94857683e-05  5.88023603e-05  2.25732445e-04\n",
      "   1.06611702e-04 -1.04972979e-04]]\n",
      "5.478426563594351e-08\n",
      "Epoch: 0 \tBatch: 33 \tLoss: 5.478426563594351e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1.09558167e-04  1.95158409e-04 -5.98383594e-05  4.78149823e-04\n",
      "  -1.91081994e-04 -3.23250137e-04]]\n",
      "3.493602642839129e-08\n",
      "Epoch: 0 \tBatch: 34 \tLoss: 3.493602642839129e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.25139158e-04 -5.57351255e-06  1.14296602e-04  3.28166402e-05\n",
      "  -1.24685786e-04 -1.94615060e-04]]\n",
      "9.520028834547333e-09\n",
      "Epoch: 0 \tBatch: 35 \tLoss: 9.520028834547333e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.74167307e-05  1.44143466e-04  4.44452158e-05 -2.53991079e-05\n",
      "   1.41802057e-04 -1.38013230e-04]]\n",
      "Validating\n",
      "4.581343113994544e-08\n",
      "val_loss: 4.581343113994544e-08\n",
      "6.835836093113141e-08\n",
      "val_loss: 6.835836093113141e-08\n",
      "2.949989129519052e-08\n",
      "val_loss: 2.949989129519052e-08\n",
      "2.593304226298318e-09\n",
      "val_loss: 2.593304226298318e-09\n",
      "3.691212934658338e-09\n",
      "val_loss: 3.691212934658338e-09\n",
      "3.3362620309644e-08\n",
      "val_loss: 3.3362620309644e-08\n",
      "3.631069032742231e-08\n",
      "val_loss: 3.631069032742231e-08\n",
      "4.7135984195805993e-08\n",
      "val_loss: 4.7135984195805993e-08\n",
      "1.0805379334567919e-08\n",
      "val_loss: 1.0805379334567919e-08\n",
      "4.74182974646884e-08\n",
      "val_loss: 4.74182974646884e-08\n",
      "7.34837607023992e-09\n",
      "Epoch: 0 \tBatch: 36 \tLoss: 7.34837607023992e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1.33724250e-04  1.51218566e-04  2.83796961e-05 -9.35865904e-05\n",
      "  -5.51191889e-05  3.71237632e-05]]\n",
      "4.630077561977742e-08\n",
      "Epoch: 0 \tBatch: 37 \tLoss: 4.630077561977742e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.60317000e-04  4.76792194e-05  1.09386605e-04 -1.04687962e-04\n",
      "  -6.12934806e-04 -2.48682594e-04]]\n",
      "3.543463756429207e-08\n",
      "Epoch: 0 \tBatch: 38 \tLoss: 3.543463756429207e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 9.31515982e-05  4.70229432e-05 -2.22771127e-04  2.60776190e-04\n",
      "   1.40429784e-04 -3.10661230e-04]]\n",
      "4.5873358696382425e-12\n",
      "Epoch: 0 \tBatch: 39 \tLoss: 4.5873358696382425e-12\n",
      "z (1, 1)\n",
      "(1, 6)\n",
      "[[-6.82470533e-07  2.89077609e-07 -1.24339160e-06 -4.13827167e-07\n",
      "   2.18590714e-07 -4.12204679e-07]]\n",
      "3.139684696216578e-08\n",
      "Epoch: 0 \tBatch: 40 \tLoss: 3.139684696216578e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[-1.51622155e-05  2.52928661e-04 -1.48513577e-04  2.41772132e-04\n",
      "  -3.79610833e-04 -2.01822508e-04]]\n",
      "Validating\n",
      "2.619470533429069e-09\n",
      "val_loss: 2.619470533429069e-09\n",
      "7.694841960878732e-09\n",
      "val_loss: 7.694841960878732e-09\n",
      "5.0103949063763975e-08\n",
      "val_loss: 5.0103949063763975e-08\n",
      "3.195268878489874e-08\n",
      "val_loss: 3.195268878489874e-08\n",
      "1.990215068888325e-08\n",
      "val_loss: 1.990215068888325e-08\n",
      "3.5684405868728535e-08\n",
      "val_loss: 3.5684405868728535e-08\n",
      "3.334430087278058e-08\n",
      "val_loss: 3.334430087278058e-08\n",
      "2.3178649786300125e-08\n",
      "val_loss: 2.3178649786300125e-08\n",
      "2.7231688700389285e-08\n",
      "val_loss: 2.7231688700389285e-08\n",
      "3.481589968713879e-09\n",
      "val_loss: 3.481589968713879e-09\n",
      "5.569279524774323e-09\n",
      "Epoch: 0 \tBatch: 41 \tLoss: 5.569279524774323e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 4.74631455e-05 -1.31782421e-04 -6.14714583e-05  3.49793084e-05\n",
      "   1.19681946e-04 -9.53170072e-05]]\n",
      "2.572369159938018e-08\n",
      "Epoch: 0 \tBatch: 42 \tLoss: 2.572369159938018e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 0.00011082  0.00029922 -0.0004013   0.00012272  0.00012525 -0.0001361 ]]\n",
      "2.323673864208223e-08\n",
      "Epoch: 0 \tBatch: 43 \tLoss: 2.323673864208223e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1.11583530e-04  3.13378568e-04  1.55997535e-04  2.21374057e-04\n",
      "   5.03433370e-05 -2.00542822e-04]]\n",
      "2.0595310641866327e-08\n",
      "Epoch: 0 \tBatch: 44 \tLoss: 2.0595310641866327e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 2.43673179e-04 -8.83726796e-05  8.49554983e-05  1.31102354e-04\n",
      "  -3.87026015e-05 -1.47290889e-04]]\n",
      "2.36682189662876e-08\n",
      "Epoch: 0 \tBatch: 45 \tLoss: 2.36682189662876e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 3.39333205e-04 -3.15276402e-05  1.50804572e-04  6.12213909e-05\n",
      "  -1.60645623e-04 -1.08595635e-04]]\n",
      "Validating\n",
      "2.000445515704284e-09\n",
      "val_loss: 2.000445515704284e-09\n",
      "2.323813963481784e-08\n",
      "val_loss: 2.323813963481784e-08\n",
      "1.670591857844355e-08\n",
      "val_loss: 1.670591857844355e-08\n",
      "3.8114968891226683e-08\n",
      "val_loss: 3.8114968891226683e-08\n",
      "2.9697913474143932e-08\n",
      "val_loss: 2.9697913474143932e-08\n",
      "1.081154017350305e-09\n",
      "val_loss: 1.081154017350305e-09\n",
      "2.1561131139970986e-08\n",
      "val_loss: 2.1561131139970986e-08\n",
      "1.4554179556426377e-08\n",
      "val_loss: 1.4554179556426377e-08\n",
      "2.7283340101729262e-08\n",
      "val_loss: 2.7283340101729262e-08\n",
      "6.194481466650801e-09\n",
      "val_loss: 6.194481466650801e-09\n",
      "1.204679673629391e-08\n",
      "Epoch: 0 \tBatch: 46 \tLoss: 1.204679673629391e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1.05364867e-05  1.92466595e-04  1.55187458e-05  2.75182906e-05\n",
      "  -1.11142180e-04 -1.38264741e-04]]\n",
      "7.944851807999467e-09\n",
      "Epoch: 0 \tBatch: 47 \tLoss: 7.944851807999467e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 8.25325876e-05  9.32064939e-05  2.92307161e-05  5.16555926e-05\n",
      "   6.45583665e-05 -7.86426820e-05]]\n",
      "1.5896520055006587e-08\n",
      "Epoch: 0 \tBatch: 48 \tLoss: 1.5896520055006587e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1.44316138e-04  1.02664381e-05 -1.31149781e-04  1.45542396e-04\n",
      "  -1.02758381e-04 -6.20028137e-05]]\n",
      "6.065314215039464e-09\n",
      "Epoch: 0 \tBatch: 49 \tLoss: 6.065314215039464e-09\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 8.97719205e-05  5.75655155e-05  1.04378712e-04  8.82318283e-05\n",
      "  -1.15691609e-04 -1.59871129e-06]]\n",
      "1.1179197761410041e-08\n",
      "Epoch: 0 \tBatch: 50 \tLoss: 1.1179197761410041e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 1.25205324e-04  2.40693368e-04  3.09404014e-05  2.59925948e-05\n",
      "  -8.37581876e-05 -5.86430563e-05]]\n",
      "Validating\n",
      "2.1791668173135352e-08\n",
      "val_loss: 2.1791668173135352e-08\n",
      "1.849371657062416e-08\n",
      "val_loss: 1.849371657062416e-08\n",
      "1.9601231788599296e-08\n",
      "val_loss: 1.9601231788599296e-08\n",
      "1.9544926009292158e-09\n",
      "val_loss: 1.9544926009292158e-09\n",
      "1.2934346699726374e-08\n",
      "val_loss: 1.2934346699726374e-08\n",
      "2.0863295276387293e-09\n",
      "val_loss: 2.0863295276387293e-09\n",
      "6.143899532138167e-09\n",
      "val_loss: 6.143899532138167e-09\n",
      "2.707446202947775e-08\n",
      "val_loss: 2.707446202947775e-08\n",
      "1.0118911806141898e-08\n",
      "val_loss: 1.0118911806141898e-08\n",
      "2.7031644229698887e-08\n",
      "val_loss: 2.7031644229698887e-08\n",
      "1.056310139547179e-08\n",
      "Epoch: 0 \tBatch: 51 \tLoss: 1.056310139547179e-08\n",
      "z (1, 5)\n",
      "(5, 6)\n",
      "[[ 8.24585536e-05  1.57156801e-04 -4.02913677e-05  1.45937952e-04\n",
      "  -1.94345457e-05 -6.75732950e-05]]\n",
      "[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
      "[2.461928558281022e-07, 1.883467168863435e-07, 1.3652669873960694e-07, 1.0303299286685585e-07, 7.535425455694285e-08, 5.396383818964043e-08, 4.043119938784235e-08, 3.249891721593526e-08, 2.3519373622876617e-08, 1.80431672376464e-08, 1.4723070295810982e-08]\n",
      "11\n",
      "11\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAERCAYAAAB4jRxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, epochs=1):\n",
    "    \n",
    "    e = []        # just a list to store the batch_idxs where validation happened\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            loss = loss_fn(model, batch)\n",
    "            print('Epoch:', epoch, '\\tBatch:', batch_idx, '\\tLoss:', loss)\n",
    "            w_grad, b_grad = loss_fn(model, batch, gradient=True)\n",
    "            optimiser.step((w_grad, b_grad))\n",
    "            \n",
    "            train_losses.append(loss)\n",
    "\n",
    "            if batch_idx % 5 == 0:\n",
    "                print('\\tValidating')\n",
    "                batch_losses = []\n",
    "                for val_batch in val_loader:\n",
    "                    l = loss_fn(model, val_batch)\n",
    "                    batch_losses.append(l)\n",
    "                avg_batch_loss = np.mean(batch_losses)\n",
    "                print('\\tAverage batch loss:', )\n",
    "                e.append(batch_idx)\n",
    "                val_losses.append(avg_batch_loss)\n",
    "                \n",
    "    plt.plot(train_losses, 'b')\n",
    "    plt.plot(e, val_losses)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "            \n",
    "train(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing the model\n",
    "\n",
    "Now we have seen our model learn from the training set, validated this with the validation set and used that to adjust the hyperparameters. Now we need to test the model by seeing how it performs on unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
